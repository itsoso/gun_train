"""
æªæ¢°åŠ¨ä½œæ¨¡å‹è®­ç»ƒæ¨¡å—
æ”¶é›†è®­ç»ƒæ•°æ®ã€è®­ç»ƒè‡ªå®šä¹‰åŠ¨ä½œè¯†åˆ«æ¨¡å‹
"""

import cv2
import numpy as np
from typing import Dict, List, Optional, Tuple, Callable
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
import threading
import queue
import time
import json
import os
import pickle
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ActionLabel(Enum):
    """åŠ¨ä½œæ ‡ç­¾"""
    IDLE = "idle"                           # ç©ºé—²
    READY_STANCE = "ready_stance"           # å‡†å¤‡å§¿åŠ¿
    DRAW_GUN = "draw_gun"                   # æ‹”æª
    AIM = "aim"                             # ç„å‡†
    FIRE = "fire"                           # å°„å‡»
    RELOAD = "reload"                       # è£…å¼¹
    HOLSTER = "holster"                     # æ”¶æª
    
    # é”™è¯¯åŠ¨ä½œ
    WRONG_GRIP = "wrong_grip"               # æ¡æªé”™è¯¯
    FINGER_ON_TRIGGER = "finger_on_trigger" # æ‰‹æŒ‡è¯¯è§¦æ‰³æœº
    MUZZLE_UNSAFE = "muzzle_unsafe"         # æªå£ä¸å®‰å…¨
    UNSTABLE_STANCE = "unstable_stance"     # ç«™å§¿ä¸ç¨³


@dataclass
class TrainingSample:
    """è®­ç»ƒæ ·æœ¬"""
    sample_id: str
    timestamp: datetime
    
    # å›¾åƒæ•°æ®
    frame: Optional[np.ndarray] = None
    frame_path: str = ""
    
    # å§¿æ€æ•°æ®
    keypoints: List[Dict] = field(default_factory=list)
    
    # æ ‡ç­¾
    action_label: ActionLabel = ActionLabel.IDLE
    quality_score: float = 0.0  # åŠ¨ä½œè´¨é‡åˆ† 0-100
    
    # å…ƒæ•°æ®
    student_id: Optional[int] = None
    workstation_id: Optional[int] = None
    instructor_id: Optional[int] = None
    notes: str = ""
    
    def to_dict(self) -> Dict:
        return {
            "sample_id": self.sample_id,
            "timestamp": self.timestamp.isoformat(),
            "frame_path": self.frame_path,
            "keypoints": self.keypoints,
            "action_label": self.action_label.value,
            "quality_score": self.quality_score,
            "student_id": self.student_id,
            "workstation_id": self.workstation_id,
            "instructor_id": self.instructor_id,
            "notes": self.notes
        }
    
    @classmethod
    def from_dict(cls, data: Dict) -> "TrainingSample":
        return cls(
            sample_id=data["sample_id"],
            timestamp=datetime.fromisoformat(data["timestamp"]),
            frame_path=data.get("frame_path", ""),
            keypoints=data.get("keypoints", []),
            action_label=ActionLabel(data["action_label"]),
            quality_score=data.get("quality_score", 0.0),
            student_id=data.get("student_id"),
            workstation_id=data.get("workstation_id"),
            instructor_id=data.get("instructor_id"),
            notes=data.get("notes", "")
        )


@dataclass
class DatasetInfo:
    """æ•°æ®é›†ä¿¡æ¯"""
    name: str
    created_at: datetime
    updated_at: datetime
    total_samples: int
    label_distribution: Dict[str, int]
    train_samples: int = 0
    val_samples: int = 0
    test_samples: int = 0


class DataCollector:
    """è®­ç»ƒæ•°æ®æ”¶é›†å™¨"""
    
    def __init__(
        self,
        output_dir: str = "training_data",
        pose_detector = None
    ):
        """
        Args:
            output_dir: æ•°æ®è¾“å‡ºç›®å½•
            pose_detector: å§¿æ€æ£€æµ‹å™¨ï¼ˆå¯é€‰ï¼‰
        """
        self.output_dir = output_dir
        self.pose_detector = pose_detector
        
        # åˆ›å»ºç›®å½•ç»“æ„
        self.images_dir = os.path.join(output_dir, "images")
        self.labels_dir = os.path.join(output_dir, "labels")
        os.makedirs(self.images_dir, exist_ok=True)
        os.makedirs(self.labels_dir, exist_ok=True)
        
        # æ ·æœ¬è®¡æ•°
        self.sample_count = 0
        self.samples: List[TrainingSample] = []
        
        # æ”¶é›†çŠ¶æ€
        self.is_collecting = False
        self.current_label = ActionLabel.IDLE
        self.current_quality = 80.0
    
    def set_label(self, label: ActionLabel, quality: float = 80.0):
        """è®¾ç½®å½“å‰æ ‡ç­¾"""
        self.current_label = label
        self.current_quality = quality
        logger.info(f"ğŸ“ æ ‡ç­¾è®¾ç½®ä¸º: {label.value}, è´¨é‡åˆ†: {quality}")
    
    def collect_frame(
        self,
        frame: np.ndarray,
        keypoints: Optional[List[Dict]] = None,
        student_id: Optional[int] = None,
        workstation_id: Optional[int] = None,
        notes: str = ""
    ) -> TrainingSample:
        """
        æ”¶é›†ä¸€å¸§ä½œä¸ºè®­ç»ƒæ ·æœ¬
        
        Args:
            frame: å›¾åƒå¸§
            keypoints: å§¿æ€å…³é”®ç‚¹ï¼ˆå¯é€‰ï¼Œå¦‚æœæœ‰pose_detectorä¼šè‡ªåŠ¨æ£€æµ‹ï¼‰
            student_id: å­¦å‘˜ID
            workstation_id: å·¥ä½ID
            notes: å¤‡æ³¨
            
        Returns:
            è®­ç»ƒæ ·æœ¬
        """
        self.sample_count += 1
        sample_id = f"sample_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{self.sample_count:06d}"
        
        # ä¿å­˜å›¾åƒ
        frame_filename = f"{sample_id}.jpg"
        frame_path = os.path.join(self.images_dir, frame_filename)
        cv2.imwrite(frame_path, frame)
        
        # æ£€æµ‹å§¿æ€ï¼ˆå¦‚æœæä¾›äº†æ£€æµ‹å™¨ä¸”æ²¡æœ‰å…³é”®ç‚¹ï¼‰
        if keypoints is None and self.pose_detector:
            keypoints = self.pose_detector.detect(frame)
            if keypoints:
                keypoints = keypoints.to_dict() if hasattr(keypoints, 'to_dict') else []
        
        # åˆ›å»ºæ ·æœ¬
        sample = TrainingSample(
            sample_id=sample_id,
            timestamp=datetime.now(),
            frame_path=frame_path,
            keypoints=keypoints or [],
            action_label=self.current_label,
            quality_score=self.current_quality,
            student_id=student_id,
            workstation_id=workstation_id,
            notes=notes
        )
        
        self.samples.append(sample)
        
        # ä¿å­˜æ ‡ç­¾
        label_path = os.path.join(self.labels_dir, f"{sample_id}.json")
        with open(label_path, 'w', encoding='utf-8') as f:
            json.dump(sample.to_dict(), f, indent=2, ensure_ascii=False)
        
        logger.debug(f"ğŸ“¸ æ”¶é›†æ ·æœ¬: {sample_id}, æ ‡ç­¾: {self.current_label.value}")
        
        return sample
    
    def start_auto_collect(
        self,
        camera,
        interval: float = 0.5,
        duration: Optional[float] = None
    ):
        """
        è‡ªåŠ¨æ”¶é›†æ¨¡å¼
        
        Args:
            camera: æ‘„åƒå¤´å®ä¾‹
            interval: é‡‡é›†é—´éš”ï¼ˆç§’ï¼‰
            duration: æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰ï¼ŒNoneè¡¨ç¤ºæŒç»­åˆ°æ‰‹åŠ¨åœæ­¢
        """
        self.is_collecting = True
        start_time = time.time()
        
        logger.info(f"ğŸ¬ å¼€å§‹è‡ªåŠ¨æ”¶é›†, é—´éš”: {interval}ç§’")
        
        while self.is_collecting:
            if duration and (time.time() - start_time) > duration:
                break
            
            frame = camera.get_frame()
            if frame is not None:
                if hasattr(frame, 'frame'):
                    frame = frame.frame
                self.collect_frame(frame)
            
            time.sleep(interval)
        
        logger.info(f"â¹ï¸ è‡ªåŠ¨æ”¶é›†ç»“æŸ, å…±æ”¶é›† {len(self.samples)} ä¸ªæ ·æœ¬")
    
    def stop_auto_collect(self):
        """åœæ­¢è‡ªåŠ¨æ”¶é›†"""
        self.is_collecting = False
    
    def get_dataset_info(self) -> DatasetInfo:
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        label_dist = {}
        for sample in self.samples:
            label = sample.action_label.value
            label_dist[label] = label_dist.get(label, 0) + 1
        
        return DatasetInfo(
            name=os.path.basename(self.output_dir),
            created_at=self.samples[0].timestamp if self.samples else datetime.now(),
            updated_at=self.samples[-1].timestamp if self.samples else datetime.now(),
            total_samples=len(self.samples),
            label_distribution=label_dist
        )
    
    def export_dataset(self, output_file: str = "dataset.json"):
        """å¯¼å‡ºæ•°æ®é›†"""
        dataset = {
            "info": {
                "name": os.path.basename(self.output_dir),
                "created_at": datetime.now().isoformat(),
                "total_samples": len(self.samples)
            },
            "samples": [s.to_dict() for s in self.samples]
        }
        
        output_path = os.path.join(self.output_dir, output_file)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“¦ æ•°æ®é›†å·²å¯¼å‡º: {output_path}")
        return output_path
    
    def load_dataset(self, dataset_file: str):
        """åŠ è½½æ•°æ®é›†"""
        with open(dataset_file, 'r', encoding='utf-8') as f:
            dataset = json.load(f)
        
        self.samples = [TrainingSample.from_dict(s) for s in dataset["samples"]]
        logger.info(f"ğŸ“‚ æ•°æ®é›†å·²åŠ è½½: {len(self.samples)} ä¸ªæ ·æœ¬")


class ActionClassifierTrainer:
    """åŠ¨ä½œåˆ†ç±»å™¨è®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model_type: str = "mlp",  # mlp, lstm, transformer
        input_dim: int = 33 * 4,  # 33ä¸ªå…³é”®ç‚¹ Ã— 4ä¸ªå€¼(x,y,z,vis)
        hidden_dims: List[int] = [256, 128, 64],
        num_classes: int = len(ActionLabel)
    ):
        """
        Args:
            model_type: æ¨¡å‹ç±»å‹
            input_dim: è¾“å…¥ç»´åº¦
            hidden_dims: éšè—å±‚ç»´åº¦
            num_classes: ç±»åˆ«æ•°
        """
        self.model_type = model_type
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.num_classes = num_classes
        
        self.model = None
        self.label_encoder = {label.value: i for i, label in enumerate(ActionLabel)}
        self.label_decoder = {i: label.value for i, label in enumerate(ActionLabel)}
        
        # è®­ç»ƒå†å²
        self.training_history = {
            "train_loss": [],
            "val_loss": [],
            "train_acc": [],
            "val_acc": []
        }
    
    def prepare_data(
        self,
        samples: List[TrainingSample],
        train_ratio: float = 0.7,
        val_ratio: float = 0.15
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®
        
        Args:
            samples: æ ·æœ¬åˆ—è¡¨
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            
        Returns:
            (X_train, y_train, X_val, y_val, X_test, y_test)
        """
        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        X = []
        y = []
        
        for sample in samples:
            if not sample.keypoints:
                continue
            
            # å±•å¹³å…³é”®ç‚¹
            features = []
            for kp in sample.keypoints[:33]:  # æœ€å¤š33ä¸ªå…³é”®ç‚¹
                if isinstance(kp, dict):
                    features.extend([
                        kp.get('x', 0),
                        kp.get('y', 0),
                        kp.get('z', 0),
                        kp.get('visibility', 0)
                    ])
                else:
                    features.extend([0, 0, 0, 0])
            
            # å¡«å……åˆ°å›ºå®šç»´åº¦
            while len(features) < self.input_dim:
                features.append(0)
            features = features[:self.input_dim]
            
            X.append(features)
            y.append(self.label_encoder[sample.action_label.value])
        
        X = np.array(X, dtype=np.float32)
        y = np.array(y, dtype=np.int64)
        
        # æ‰“ä¹±æ•°æ®
        indices = np.random.permutation(len(X))
        X = X[indices]
        y = y[indices]
        
        # åˆ’åˆ†æ•°æ®é›†
        n = len(X)
        train_end = int(n * train_ratio)
        val_end = int(n * (train_ratio + val_ratio))
        
        X_train, y_train = X[:train_end], y[:train_end]
        X_val, y_val = X[train_end:val_end], y[train_end:val_end]
        X_test, y_test = X[val_end:], y[val_end:]
        
        logger.info(f"ğŸ“Š æ•°æ®å‡†å¤‡å®Œæˆ: è®­ç»ƒ{len(X_train)}, éªŒè¯{len(X_val)}, æµ‹è¯•{len(X_test)}")
        
        return X_train, y_train, X_val, y_val, X_test, y_test
    
    def build_model(self):
        """æ„å»ºæ¨¡å‹ï¼ˆä½¿ç”¨numpyå®ç°ç®€å•MLPï¼‰"""
        # åˆå§‹åŒ–æƒé‡
        np.random.seed(42)
        
        layers = [self.input_dim] + self.hidden_dims + [self.num_classes]
        self.weights = []
        self.biases = []
        
        for i in range(len(layers) - 1):
            # Xavieråˆå§‹åŒ–
            w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])
            b = np.zeros(layers[i+1])
            self.weights.append(w)
            self.biases.append(b)
        
        logger.info(f"ğŸ—ï¸ æ¨¡å‹æ„å»ºå®Œæˆ: {layers}")
    
    def relu(self, x):
        return np.maximum(0, x)
    
    def relu_derivative(self, x):
        return (x > 0).astype(float)
    
    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)
    
    def forward(self, X: np.ndarray) -> Tuple[np.ndarray, List[np.ndarray]]:
        """å‰å‘ä¼ æ’­"""
        activations = [X]
        
        for i, (w, b) in enumerate(zip(self.weights, self.biases)):
            z = activations[-1] @ w + b
            
            if i < len(self.weights) - 1:
                a = self.relu(z)
            else:
                a = self.softmax(z)
            
            activations.append(a)
        
        return activations[-1], activations
    
    def compute_loss(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:
        """è®¡ç®—äº¤å‰ç†µæŸå¤±"""
        n = len(y_true)
        # é¿å…log(0)
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
        loss = -np.sum(np.log(y_pred[np.arange(n), y_true])) / n
        return loss
    
    def backward(
        self,
        X: np.ndarray,
        y: np.ndarray,
        activations: List[np.ndarray],
        learning_rate: float
    ):
        """åå‘ä¼ æ’­"""
        n = len(y)
        
        # è¾“å‡ºå±‚æ¢¯åº¦
        y_onehot = np.zeros((n, self.num_classes))
        y_onehot[np.arange(n), y] = 1
        
        delta = activations[-1] - y_onehot
        
        # åå‘ä¼ æ’­
        for i in range(len(self.weights) - 1, -1, -1):
            dw = activations[i].T @ delta / n
            db = np.mean(delta, axis=0)
            
            # æ›´æ–°æƒé‡
            self.weights[i] -= learning_rate * dw
            self.biases[i] -= learning_rate * db
            
            if i > 0:
                delta = (delta @ self.weights[i].T) * self.relu_derivative(activations[i])
    
    def train(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
        epochs: int = 100,
        batch_size: int = 32,
        learning_rate: float = 0.001,
        early_stopping_patience: int = 10
    ):
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒæ ‡ç­¾
            X_val: éªŒè¯ç‰¹å¾
            y_val: éªŒè¯æ ‡ç­¾
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹å¤§å°
            learning_rate: å­¦ä¹ ç‡
            early_stopping_patience: æ—©åœè€å¿ƒå€¼
        """
        if self.weights is None:
            self.build_model()
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒ: epochs={epochs}, batch_size={batch_size}, lr={learning_rate}")
        
        for epoch in range(epochs):
            # æ‰“ä¹±è®­ç»ƒæ•°æ®
            indices = np.random.permutation(len(X_train))
            X_train_shuffled = X_train[indices]
            y_train_shuffled = y_train[indices]
            
            # æ‰¹é‡è®­ç»ƒ
            train_losses = []
            for i in range(0, len(X_train), batch_size):
                X_batch = X_train_shuffled[i:i+batch_size]
                y_batch = y_train_shuffled[i:i+batch_size]
                
                # å‰å‘ä¼ æ’­
                y_pred, activations = self.forward(X_batch)
                loss = self.compute_loss(y_pred, y_batch)
                train_losses.append(loss)
                
                # åå‘ä¼ æ’­
                self.backward(X_batch, y_batch, activations, learning_rate)
            
            # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
            train_loss = np.mean(train_losses)
            y_train_pred, _ = self.forward(X_train)
            train_acc = np.mean(np.argmax(y_train_pred, axis=1) == y_train)
            
            # è®¡ç®—éªŒè¯æŒ‡æ ‡
            y_val_pred, _ = self.forward(X_val)
            val_loss = self.compute_loss(y_val_pred, y_val)
            val_acc = np.mean(np.argmax(y_val_pred, axis=1) == y_val)
            
            # è®°å½•å†å²
            self.training_history["train_loss"].append(train_loss)
            self.training_history["val_loss"].append(val_loss)
            self.training_history["train_acc"].append(train_acc)
            self.training_history["val_acc"].append(val_acc)
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 10 == 0:
                logger.info(
                    f"Epoch {epoch+1}/{epochs} - "
                    f"train_loss: {train_loss:.4f}, train_acc: {train_acc:.4f}, "
                    f"val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}"
                )
            
            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self.best_weights = [w.copy() for w in self.weights]
                self.best_biases = [b.copy() for b in self.biases]
            else:
                patience_counter += 1
                if patience_counter >= early_stopping_patience:
                    logger.info(f"ğŸ›‘ æ—©åœäº epoch {epoch+1}")
                    # æ¢å¤æœ€ä½³æ¨¡å‹
                    self.weights = self.best_weights
                    self.biases = self.best_biases
                    break
        
        logger.info(f"âœ… è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        y_pred, _ = self.forward(X)
        return np.argmax(y_pred, axis=1)
    
    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹æ¦‚ç‡"""
        y_pred, _ = self.forward(X)
        return y_pred
    
    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """è¯„ä¼°æ¨¡å‹"""
        y_pred = self.predict(X)
        accuracy = np.mean(y_pred == y)
        
        # è®¡ç®—æ¯ç±»å‡†ç¡®ç‡
        class_accuracy = {}
        for label_name, label_idx in self.label_encoder.items():
            mask = y == label_idx
            if np.sum(mask) > 0:
                class_accuracy[label_name] = np.mean(y_pred[mask] == y[mask])
        
        return {
            "accuracy": accuracy,
            "class_accuracy": class_accuracy
        }
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            "model_type": self.model_type,
            "input_dim": self.input_dim,
            "hidden_dims": self.hidden_dims,
            "num_classes": self.num_classes,
            "weights": [w.tolist() for w in self.weights],
            "biases": [b.tolist() for b in self.biases],
            "label_encoder": self.label_encoder,
            "label_decoder": self.label_decoder,
            "training_history": self.training_history
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {filepath}")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.model_type = model_data["model_type"]
        self.input_dim = model_data["input_dim"]
        self.hidden_dims = model_data["hidden_dims"]
        self.num_classes = model_data["num_classes"]
        self.weights = [np.array(w) for w in model_data["weights"]]
        self.biases = [np.array(b) for b in model_data["biases"]]
        self.label_encoder = model_data["label_encoder"]
        self.label_decoder = model_data["label_decoder"]
        self.training_history = model_data.get("training_history", {})
        
        logger.info(f"ğŸ“‚ æ¨¡å‹å·²åŠ è½½: {filepath}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ•°æ®æ”¶é›†å™¨
    collector = DataCollector(output_dir="training_data")
    
    # æ¨¡æ‹Ÿæ”¶é›†ä¸€äº›æ•°æ®
    for i in range(100):
        # æ¨¡æ‹Ÿå¸§
        frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        
        # æ¨¡æ‹Ÿå…³é”®ç‚¹
        keypoints = [
            {"x": np.random.rand(), "y": np.random.rand(), "z": np.random.rand() * 0.1, "visibility": 0.9}
            for _ in range(33)
        ]
        
        # è®¾ç½®æ ‡ç­¾
        labels = list(ActionLabel)
        collector.set_label(labels[i % len(labels)], quality=70 + np.random.rand() * 30)
        
        # æ”¶é›†æ ·æœ¬
        collector.collect_frame(frame, keypoints=keypoints)
    
    # å¯¼å‡ºæ•°æ®é›†
    collector.export_dataset()
    
    # è®­ç»ƒæ¨¡å‹
    trainer = ActionClassifierTrainer()
    X_train, y_train, X_val, y_val, X_test, y_test = trainer.prepare_data(collector.samples)
    
    trainer.build_model()
    trainer.train(X_train, y_train, X_val, y_val, epochs=50)
    
    # è¯„ä¼°
    eval_result = trainer.evaluate(X_test, y_test)
    print(f"\nğŸ“Š æµ‹è¯•é›†å‡†ç¡®ç‡: {eval_result['accuracy']:.4f}")
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_model("action_classifier.pkl")

